# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

**PSY Agents NO-AUG** is a baseline implementation for clinical text analysis (Criteria and Evidence extraction from DSM-5 diagnostic posts) **without data augmentation**. This is a control repository for comparing against augmentation-enhanced models.

**Critical Principles:**

1. **STRICT Field Separation** (enforced by assertions):
   - **Criteria Task**: Uses ONLY `status` field from annotations
   - **Evidence Task**: Uses ONLY `cases` field from annotations
   - Any code violating this separation will fail with `AssertionError`

2. **NO Augmentation** (despite some residual code):
   - This is a baseline NO-AUG control repository
   - ‚ö†Ô∏è WARNING: `src/psy_agents_noaug/augmentation/` exists but is **UNUSED** (100KB of dead code)
   - Dependencies nlpaug/textattack are listed but not used in training

3. **Duplicate Architecture Warning**:
   - ‚ö†Ô∏è **TWO identical implementations** exist (904KB duplication):
     - `src/Project/` (376KB) - Used by standalone scripts
     - `src/psy_agents_noaug/architectures/` (528KB) - Not used by CLI
   - Only modify one to avoid divergence
   - See `CODEBASE_STRUCTURE_ANALYSIS.md` for consolidation plan

**Development Environment:** Uses VS Code Dev Containers (`.devcontainer/`) with CUDA 12.1, Python 3.10, Poetry, PyTorch.

## Essential Commands

### Development Environment
```bash
# Open in VS Code Dev Container (recommended)
# 1. Install "Dev Containers" extension in VS Code
# 2. Open folder in VS Code
# 3. Click "Reopen in Container" when prompted
# Container includes: CUDA 12.1, Python 3.10, Poetry, PyTorch

# Or develop locally:
make setup              # Full setup: dependencies + pre-commit + tests
make install            # Install dependencies only
poetry install          # Direct poetry install
```

### Data Preparation
```bash
# Generate ground truth from HuggingFace (recommended)
make groundtruth
# Or: python -m psy_agents_noaug.cli make_groundtruth data=hf_redsm5

# Generate from local CSV
make groundtruth-local
```

### Training
```bash
# Train with defaults (criteria, roberta_base)
make train

# Train specific task/model
make train TASK=criteria MODEL=roberta_base

# Or use CLI directly with Hydra overrides
python -m psy_agents_noaug.cli train \
    task=criteria \
    model=roberta_base \
    training.num_epochs=20 \
    training.batch_size=32

# Train Criteria architecture standalone
python scripts/train_criteria.py

# Evaluate Criteria model
python scripts/eval_criteria.py checkpoint=outputs/checkpoints/best_checkpoint.pt
```

### Hyperparameter Optimization (HPO) üöÄ PRODUCTION READY

**TWO HPO SYSTEMS** (both use real redsm5 data):

**1. Multi-Stage HPO** (Progressive refinement):
```bash
# Single architecture (criteria, evidence, share, or joint)
make hpo-s0 HPO_TASK=criteria    # Stage 0: Sanity (8 trials)
make hpo-s1 HPO_TASK=criteria    # Stage 1: Coarse (20 trials)
make hpo-s2 HPO_TASK=criteria    # Stage 2: Fine (50 trials)
make refit HPO_TASK=criteria     # Stage 3: Refit on train+val

# Or run all stages for one architecture
make full-hpo HPO_TASK=criteria

# Run ALL architectures sequentially
make full-hpo-all
```

**2. Maximal HPO** (Single large run, 600-1200 trials):
```bash
# Single architecture
make tune-criteria-max    # 800 trials
make tune-evidence-max    # 1200 trials
make tune-share-max       # 600 trials
make tune-joint-max       # 600 trials

# Run ALL architectures sequentially
make maximal-hpo-all

# Or use wrapper script for custom settings
python scripts/run_all_hpo.py --mode maximal --parallel 4
```

See `docs/HPO_GUIDE.md` for comprehensive documentation.

### Testing and Quality
```bash
make test               # Run all tests
make test-cov           # With coverage report
make test-groundtruth   # Test strict validation rules only

make lint               # Run ruff + black --check
make format             # Format with ruff + black

make pre-commit-run     # Run all pre-commit hooks

# Single test file
poetry run pytest tests/test_groundtruth.py -v
```

### MLflow and Evaluation
```bash
# Start MLflow UI
mlflow ui --backend-store-uri sqlite:///mlflow.db

# Evaluate model
make eval CHECKPOINT=outputs/checkpoints/best_checkpoint.pt

# Export metrics to CSV
make export
```

## Architecture Overview

### Four Architectures (‚ö†Ô∏è DUPLICATE IMPLEMENTATIONS)

**Current State:**
- `src/Project/` (376KB) - **ACTIVE**: Used by `train_criteria.py`, `eval_criteria.py`
- `src/psy_agents_noaug/architectures/` (528KB) - **INACTIVE**: Has train/eval engines but not used by CLI

**Architectures:**

1. **Criteria** (`criteria/`): Binary classification for criterion presence
   - Model: Transformer encoder ‚Üí pooled output ‚Üí binary classification head
   - Dataset: `CriteriaDataset` (uses ONLY `status` field)
   - Scripts: `scripts/train_criteria.py`, `scripts/eval_criteria.py` ‚úÖ Production-ready

2. **Evidence** (`evidence/`): Span extraction for supporting text
   - Model: Transformer encoder ‚Üí span prediction head (start/end positions)
   - Dataset: `EvidenceDataset` (uses ONLY `cases` field)

3. **Share** (`share/`): Shared encoder with dual heads
   - Single transformer encoder for both tasks
   - Separate classification head and span prediction head

4. **Joint** (`joint/`): Dual encoders with fusion
   - Separate encoders for criteria and evidence tasks
   - Fusion layer before evidence head

**Interface Parity (October 2025):**
- All architectures accept `head_cfg` and `task_cfg` parameters
- Standardized output keys (`"logits"` for all)
- Backward compatible with direct parameter passing

### Data Pipeline Architecture

**STRICT Validation Flow:**
```
Raw Data ‚Üí Field Map Validation ‚Üí Groundtruth Generation
                                           ‚Üì
                     +--------------------+--------------------+
                     ‚Üì                                         ‚Üì
            Criteria Groundtruth                     Evidence Groundtruth
            (status field ONLY)                      (cases field ONLY)
                     ‚Üì                                         ‚Üì
              CriteriaDataset                          EvidenceDataset
```

**Key Files:**
- `configs/data/field_map.yaml`: Defines field mappings and validation rules
- `src/psy_agents_noaug/data/groundtruth.py`: Groundtruth generation with assertions
- `src/psy_agents_noaug/data/loaders.py`: Data loading with strict validation
- `src/psy_agents_noaug/data/splits.py`: Train/val/test splitting

**Validation Guarantees:**
- `_assert_field_usage()` function fails if wrong field is accessed
- Status values normalized to binary (0/1)
- Cases parsed from JSON/list format and validated
- Tests in `tests/test_groundtruth.py` verify separation

### Training Infrastructure

**Reproducibility (NEW - 2025):**
- Enhanced seed management in `src/psy_agents_noaug/utils/reproducibility.py`
- Full determinism with `torch.use_deterministic_algorithms()`
- Hardware-optimized DataLoader settings (num_workers, pin_memory, persistent_workers)
- Mixed precision support (Float16/BFloat16 with automatic GPU detection)

**Training Scripts:**
- `scripts/train_criteria.py`: Standalone Criteria training ‚úÖ PRODUCTION-READY
- `scripts/eval_criteria.py`: Standalone Criteria evaluation ‚úÖ PRODUCTION-READY
- `scripts/train_best.py`: HPO integration router (routes to architecture-specific scripts)
- `scripts/run_hpo_stage.py`: Multi-stage HPO runner ‚úÖ REAL DATA
- `scripts/tune_max.py`: Maximal HPO runner ‚úÖ REAL DATA
- `scripts/run_all_hpo.py`: Sequential HPO wrapper for all architectures ‚úÖ NEW

**Training Configs:**
- `configs/training/default.yaml`: Standard settings with hardware optimizations
- `configs/training/optimized.yaml`: Comprehensive annotated config for max performance

**Core Training Loop:**
- `src/psy_agents_noaug/training/train_loop.py`: `Trainer` class with:
  - Mixed precision (AMP)
  - Gradient accumulation and clipping
  - Early stopping on validation metrics
  - MLflow logging
  - Checkpoint management

**Key Optimizations (2025 Best Practices):**
```yaml
# Mixed Precision
amp:
  enabled: true
  dtype: "float16"  # Use "bfloat16" for Ampere+ GPUs

# DataLoader (2-5x faster)
num_workers: 8          # Start with 2√ó CPU cores per GPU
pin_memory: true        # Always true for GPU training
persistent_workers: true
prefetch_factor: 2

# Reproducibility vs Speed
deterministic: true     # Full reproducibility (slower)
cudnn_benchmark: false  # Deterministic algorithms
```

### Configuration System (Hydra)

**Composition:**
```yaml
# configs/config.yaml
defaults:
  - data: hf_redsm5        # Data source
  - model: roberta_base     # Model architecture
  - training: default       # Training config
  - task: criteria          # Task definition
  - hpo: stage0_sanity      # HPO config
```

**Override Examples:**
```bash
# Single override
python -m psy_agents_noaug.cli train task=evidence

# Nested override
python -m psy_agents_noaug.cli train training.batch_size=32 training.optimizer.lr=3e-5

# Multiple models (multirun)
python -m psy_agents_noaug.cli train -m model=bert_base,roberta_base,deberta_v3_base
```

**Config Groups:**
- `data/`: Data sources (hf_redsm5, local_csv) + field_map.yaml
- `model/`: Model architectures (bert_base, roberta_base, deberta_v3_base)
- `training/`: Training hyperparameters
- `task/`: Task definitions (criteria, evidence)
- `hpo/`: HPO stages (stage0_sanity, stage1_coarse, stage2_fine, stage3_refit)

### CLI Architecture

**Entry Point:** `src/psy_agents_noaug/cli.py`

**Commands:**
1. `make_groundtruth`: Generate ground truth with strict validation
2. `train`: Train model with specified config
3. `hpo`: Run HPO stage (calls Optuna)
4. `refit`: Retrain best model on train+val
5. `evaluate_best`: Evaluate checkpoint on test set
6. `export_metrics`: Export MLflow metrics to CSV

**All commands use Hydra for configuration management.**

## Critical Data Rules

### Field Separation (ENFORCED)

```python
# In src/psy_agents_noaug/data/groundtruth.py
def _assert_field_usage(field_name: str, expected_field: str, operation: str):
    """Raises AssertionError if wrong field is used."""
    assert field_name == expected_field, (
        f"STRICT VALIDATION: {operation} must use '{expected_field}' field, "
        f"but '{field_name}' was provided"
    )

# Criteria uses ONLY status
_assert_field_usage(field_name, "status", "Criteria groundtruth generation")

# Evidence uses ONLY cases
_assert_field_usage(field_name, "cases", "Evidence groundtruth generation")
```

**Never mix these fields. Tests will fail if violated.**

### Field Mapping Format

```yaml
# configs/data/field_map.yaml
annotations:
  columns:
    status:
      required: true
      used_for: ["criteria"]
      type: "string"
      normalization:
        positive_values: ["positive", "present", "true", "1", 1, true]
        negative_values: ["negative", "absent", "false", "0", 0, false]

    cases:
      required: true
      used_for: ["evidence"]
      type: "json"
      structure:
        - text: "string"
        - start_char: "int"
        - end_char: "int"
```

## Key Testing Files

- `tests/test_groundtruth.py`: **Validates STRICT field separation** (highest priority)
- `tests/test_loaders.py`: Data loading validation
- `tests/test_training_smoke.py`: Training pipeline smoke tests
- `tests/test_hpo_config.py`: HPO config validation
- `tests/test_integration.py`: End-to-end workflow tests

## MLflow Tracking

**Backend:** SQLite (`mlflow.db`)
**Artifacts:** File system (`mlruns/`)

```python
# Tracking URI resolution
tracking_uri = "sqlite:///mlflow.db"
artifact_location = "./mlruns"

# Logged metrics
- Training: loss, accuracy (per step)
- Validation: loss, accuracy, F1 (macro/micro), precision, recall (per epoch)
- System: learning rate, epoch time
- Hyperparameters: all training config
```

**View UI:**
```bash
mlflow ui --backend-store-uri sqlite:///mlflow.db
```

## Project Structure Highlights

**Note:** See `OPTIMIZATION_SUMMARY.md` for recent cleanup details (removed 3 redundant docs, cleaned caches, unified Docker config).

```
NoAug_Criteria_Evidence/
‚îú‚îÄ‚îÄ .devcontainer/             # Dev Container (SINGLE source for Docker)
‚îÇ   ‚îú‚îÄ‚îÄ devcontainer.json      # VS Code settings + GPU config
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile             # CUDA 12.1 + Python 3.10 + Poetry
‚îÇ   ‚îî‚îÄ‚îÄ docker-compose.yml     # Service composition
‚îÇ
‚îú‚îÄ‚îÄ configs/                    # Hydra configs (YAML)
‚îÇ   ‚îú‚îÄ‚îÄ config.yaml            # Main composition
‚îÇ   ‚îú‚îÄ‚îÄ data/field_map.yaml    # STRICT field mappings
‚îÇ   ‚îú‚îÄ‚îÄ model/                 # Model configs
‚îÇ   ‚îú‚îÄ‚îÄ training/              # Training configs
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ default.yaml       # Standard settings
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ optimized.yaml     # Max performance settings
‚îÇ   ‚îú‚îÄ‚îÄ task/                  # Task definitions
‚îÇ   ‚îî‚îÄ‚îÄ hpo/                   # HPO stages
‚îÇ
‚îú‚îÄ‚îÄ src/psy_agents_noaug/
‚îÇ   ‚îú‚îÄ‚îÄ architectures/         # Four architectures
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ criteria/          # Binary classification
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ evidence/          # Span extraction
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ share/             # Shared encoder
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ joint/             # Dual encoders
‚îÇ   ‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ groundtruth.py     # Groundtruth with strict validation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ loaders.py         # Data loading
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ splits.py          # Train/val/test splits
‚îÇ   ‚îú‚îÄ‚îÄ training/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_loop.py      # Trainer class (AMP, early stopping)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ evaluate.py        # Evaluator class
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ reproducibility.py # Enhanced seed + hardware utils
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ mlflow_utils.py    # MLflow helpers
‚îÇ   ‚îî‚îÄ‚îÄ cli.py                 # Unified CLI
‚îÇ
‚îú‚îÄ‚îÄ src/Project/               # Architecture implementations (800KB)
‚îÇ   ‚îÇ                          # Used by: train_criteria.py, eval_criteria.py
‚îÇ   ‚îú‚îÄ‚îÄ Criteria/              # Simpler, standalone implementation
‚îÇ   ‚îú‚îÄ‚îÄ Evidence/              # Binary/multi-class classification
‚îÇ   ‚îú‚îÄ‚îÄ Joint/                 # Multi-task model
‚îÇ   ‚îî‚îÄ‚îÄ Share/                 # Shared encoder
‚îÇ
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ train_criteria.py      # ‚úÖ Production-ready Criteria training
‚îÇ   ‚îú‚îÄ‚îÄ eval_criteria.py       # ‚úÖ Production-ready Criteria evaluation
‚îÇ   ‚îú‚îÄ‚îÄ train_best.py          # HPO integration router
‚îÇ   ‚îú‚îÄ‚îÄ run_hpo_stage.py       # HPO runner
‚îÇ   ‚îî‚îÄ‚îÄ make_groundtruth.py    # Ground truth generation
‚îÇ
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ test_groundtruth.py    # ‚ö†Ô∏è CRITICAL: Tests field separation
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ TRAINING_GUIDE.md      # ‚úÖ NEW: Comprehensive training guide
‚îÇ   ‚îú‚îÄ‚îÄ TRAINING_SETUP_COMPLETE.md  # ‚úÖ NEW: Setup summary
‚îÇ   ‚îú‚îÄ‚îÄ DATA_PIPELINE_IMPLEMENTATION.md
‚îÇ   ‚îú‚îÄ‚îÄ CLI_AND_MAKEFILE_GUIDE.md
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ
‚îú‚îÄ‚îÄ Makefile                   # Convenient command shortcuts
‚îî‚îÄ‚îÄ pyproject.toml            # Poetry dependencies
```

## Known Technical Debt

**1. Duplicate Architecture Implementations (904KB)**
- `src/Project/` (376KB) - Currently ACTIVE for standalone scripts
- `src/psy_agents_noaug/architectures/` (528KB) - Has engines but UNUSED
- **Impact**: Confusion, divergence risk, wasted disk space
- **Consolidation plan**: See `CODEBASE_STRUCTURE_ANALYSIS.md` (estimated 2-4 hours)

**2. Unused Augmentation Code (100KB)**
- `src/psy_agents_noaug/augmentation/` (32KB) - Dead code
- `tests/test_augmentation_*.py` (55KB) - Dead tests
- Dependencies: nlpaug, textattack (unused)
- **Impact**: Contradicts NO-AUG mission, slower pip install
- **Removal plan**: See `CODEBASE_STRUCTURE_ANALYSIS.md` Phase 1 (estimated 1-2 hours)

**3. Hidden Production Scripts**
- `scripts/train_criteria.py` (416 lines) - ‚úÖ Production-ready but no Makefile target
- `scripts/eval_criteria.py` (306 lines) - ‚úÖ Production-ready but no Makefile target
- **Impact**: Reduced discoverability
- **Solution**: Add to Makefile or document explicitly

## Common Pitfalls

1. **Field Mixing**: Never use `status` for evidence or `cases` for criteria. Tests will fail.

2. **Config Paths**: Hydra configs are relative to `configs/`, not full paths:
   ```bash
   # ‚úì Correct
   python -m psy_agents_noaug.cli train task=criteria

   # ‚úó Wrong
   python -m psy_agents_noaug.cli train task=configs/task/criteria.yaml
   ```

3. **Poetry Environment**: Always use `poetry run` or activate poetry shell:
   ```bash
   poetry run python -m psy_agents_noaug.cli train task=criteria
   # Or
   poetry shell
   python -m psy_agents_noaug.cli train task=criteria
   ```

4. **Reproducibility Trade-off**: `deterministic=true` ensures exact reproducibility but is 20% slower. Set to `false` for production/inference.

5. **Checkpoint Paths**: Use absolute paths or paths relative to project root:
   ```bash
   # ‚úì Correct
   make eval CHECKPOINT=outputs/checkpoints/best_checkpoint.pt

   # ‚úó May fail if run from wrong directory
   make eval CHECKPOINT=best_checkpoint.pt
   ```

6. **HPO Requirements**: Stage 3 (refit) requires best config from stage 2:
   ```bash
   make hpo-s2  # Must complete first
   make refit   # Uses outputs/hpo_stage2/best_config.yaml
   ```

## Development Workflow

1. **Make Changes**: Edit code in `src/` or `configs/`
2. **Format**: `make format`
3. **Lint**: `make lint`
4. **Test**: `make test`
5. **Pre-commit**: `make pre-commit-run`
6. **Commit**: Standard git workflow

## Quick Reference

```bash
# Complete workflow from scratch
make setup                      # 1. Setup
make groundtruth                # 2. Generate data
make hpo-s0                     # 3. Sanity check
make full-hpo                   # 4. Full HPO (stages 1-3)
make eval                       # 5. Evaluate
make export                     # 6. Export results

# Development cycle
make format && make lint && make test

# View documentation
cat docs/TRAINING_GUIDE.md      # Comprehensive training guide
cat docs/CLI_AND_MAKEFILE_GUIDE.md  # CLI reference
cat docs/QUICK_START.md         # Quick start guide
```

## Recent Updates (2025)

**October 2025:**
- ‚úÖ Production-ready HPO system (multi-stage + maximal modes)
- ‚úÖ Interface parity: all architectures accept `head_cfg`/`task_cfg`
- ‚úÖ Optuna 4.5.0 compatibility (NSGAIISampler)
- ‚úÖ PyTorch 2.x AMP API migration
- ‚úÖ 67/69 tests passing (97.1%), 31% coverage
- ‚úÖ Comprehensive codebase structure analysis added

**Training Infrastructure (January 2025):**
- Enhanced reproducibility with full determinism (`torch.use_deterministic_algorithms()`)
- Hardware-optimized DataLoader (2-5x faster with persistent workers)
- Mixed precision (AMP) with Float16/BFloat16 auto-detection
- Production-ready standalone scripts: `train_criteria.py`, `eval_criteria.py`

**Project Optimization:**
- Removed 3 redundant docs (-985 lines)
- Unified Docker config (`.devcontainer/` only)
- Cleaned all cache files
- **NEW**: Added `CODEBASE_STRUCTURE_ANALYSIS.md` and `CODEBASE_QUICK_SUMMARY.txt`

**Key Documentation:**
- `docs/HPO_GUIDE.md` - HPO system comprehensive guide
- `docs/TRAINING_GUIDE.md` - Training best practices
- `docs/CLI_AND_MAKEFILE_GUIDE.md` - CLI reference
- `CODEBASE_STRUCTURE_ANALYSIS.md` - ‚≠ê Detailed code inventory and consolidation roadmap
- `OPTIMIZATION_SUMMARY.md` - Previous cleanup details
