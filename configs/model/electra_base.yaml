# ELECTRA Base Configuration
# Efficiently Learning an Encoder that Classifies Token Replacements Accurately
# Paper: https://arxiv.org/abs/2003.10555

name: "google/electra-base-discriminator"

# Model architecture
hidden_size: 768
num_attention_heads: 12
num_hidden_layers: 12
intermediate_size: 3072
max_position_embeddings: 512

# Training configuration
gradient_checkpointing: false
dropout: 0.1
attention_dropout: 0.1

# Notes:
# - ELECTRA uses discriminator for downstream tasks (not generator)
# - More sample-efficient than BERT due to replaced token detection pre-training
# - Typically achieves similar/better performance than BERT with less compute
